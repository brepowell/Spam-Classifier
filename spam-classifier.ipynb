{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breanna Powell and Melody Behdarvandian\n",
    "# CSS 576\n",
    "# Assignment 3\n",
    "\n",
    "# IDE: Visual Studio Code \n",
    "# Follow these steps if you have not used Jupyter Notebooks in VS Code before:\n",
    "# https://code.visualstudio.com/docs/languages/python\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "#                    FOLLOW THESE STEPS TO INSTALL TENSORFLOW in VS CODE\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Open Anaconda Navigator \n",
    "# Launch VS Code through Anaconda Navigator\n",
    "# Terminal > New Terminal\n",
    "#\n",
    "# https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\n",
    "# 1) Use the commands to create a tensorflow environment:\n",
    "#    $ conda create -n tf tensorflow\n",
    "#    $ conda activate tf\n",
    "#\n",
    "# https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_create-or-open-a-jupyter-notebook\n",
    "# 2) In the upper right hand corner, switch the kernel from \"base\" over to \"tf(Python 3.10.9)\"\n",
    "# This will change the kernel over to tensorflow's kernel.\n",
    "#\n",
    "# 3) Close this document and reopen it from Anaconda Navigator, but instead of \"base\" select \"tf\" from the dropdown menu\n",
    "# If you don't see \"tf\" in the dropdown menu, try closing Anaconda Navigator and reopening it.\n",
    "#\n",
    "# 4) Install the following:\n",
    "#    $ conda install ipykernel\n",
    "#    $ conda install pandas matplotlib scikit-learn seaborn\n",
    "#    $ conda install -c conda-forge tensorflow keras\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "emailData = pd.read_csv('emails.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailData.shape # Check the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are missing values (NaN or null)\n",
    "emailData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailData.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace labels with 0 for ham and 1 for spam\n",
    "emailData[\"label\"] = (emailData[\"label\"] == \"spam\").astype(int)\n",
    "\n",
    "# Check to make sure it worked\n",
    "emailData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (x) from the labels (y)\n",
    "x = emailData['email']\n",
    "y = emailData[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a count vectorizer to the training data to convert from text to token counts\n",
    "# Count vectorizer -- will it ensure the testing data has no impact on the training data's normalization?\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(strip_accents= \"unicode\", stop_words=\"english\", max_features=40) # This cuts the features to 30 words\n",
    "features = cv.fit_transform(x)\n",
    "features.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = cv.get_feature_names_out()\n",
    "\n",
    "# Features is now in csr format (rather than a series or dataframe)\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "type(features)\n",
    "x = pd.DataFrame(features.toarray()) # convert it to an dataframe instead\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure that there are no NaN's\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html\n",
    "x.isna().sum().sum()\n",
    "\n",
    "# Should print 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns = columns\n",
    "x = x.drop([\"gt\", \"lt\"], axis=1) # In the dataset we are using, &gt;  and &lt; are less than or greater than signs in html - not words\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- INITIAL FEATURE REDUCTION: THIS PART OF THE CODE MAY TAKE UP TO 30 MINUTES TO RUN ----\n",
    "# maxIndex = len(x.columns)-1\n",
    "\n",
    "# for i in range(maxIndex,0,-1):\n",
    "    \n",
    "#     if x[i].max() < 5.0:\n",
    "#         x = x.drop(x.columns[i], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = x.drop(x.columns[0], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substract the mean\n",
    "x = x-x.mean()\n",
    "x = x-x.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zVar = np.dot(x.T,x)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(zVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonalMatrix = np.diag(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newMatrix = np.dot(zVar, eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Calculate the proportion of variance explained by each feature\n",
    "sum_eigenvalues = np.sum(eigenvalues)\n",
    "\n",
    "prop_var = [i/sum_eigenvalues for i in eigenvalues]\n",
    "\n",
    "#2. Calculate the cumulative variance\n",
    "cum_var = [np.sum(prop_var[:i+1]) for i in range(len(prop_var))]\n",
    "\n",
    "\n",
    "# Plot scree plot from PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_labels = ['PC{}'.format(i+1) for i in range(len(prop_var))]\n",
    "\n",
    "plt.plot(x_labels, prop_var, marker='o', markersize=6, color='skyblue', linewidth=2, label='Proportion of variance')\n",
    "plt.plot(x_labels, cum_var, marker='o', color='orange', linewidth=2, label=\"Cumulative variance\")\n",
    "plt.legend()\n",
    "plt.title('Scree plot')\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Proportion of variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prop_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(x, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = len(x.columns);\n",
    "nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vsquare = [[0 for x in range(0,nfeatures)] for y in range(0,nfeatures)] \n",
    "\n",
    "for i in range(0,nfeatures):\n",
    "    for j in range(0,nfeatures):\n",
    "        Vsquare[i][j] = vh[i][j]**2\n",
    "        if vh[i][j]<0:\n",
    "\n",
    "            Vsquare[i][j] = Vsquare[i][j]*-1; \n",
    "        else:\n",
    "            Vsquare[i][j] = Vsquare[i][j]*1; \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import bar, grid, title, xlabel, ylabel\n",
    "\n",
    "\n",
    "bar(range(0,nfeatures), Vsquare[:][0], 0.5); # CHANGED FROM 1.5 TO 1.0\n",
    "    \n",
    "grid; \n",
    "    \n",
    "    \n",
    "xlabel('Feature index')\n",
    "    \n",
    "ylabel('Importance of feature')\n",
    "    \n",
    "\n",
    "    \n",
    "title(\"Loading Vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing data \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "train_shape = x_train.shape\n",
    "b = train_shape[1]\n",
    "input_shape = (b,)\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# https://keras.io/api/models/sequential/\n",
    "\n",
    "\n",
    "# Activation function options:\n",
    "# - relu\n",
    "# - sigmoid - good for last layer of binary classification\n",
    "# - elu - need to research this one\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape = input_shape, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid')) # Always use a sigmoid function for final layer when performing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "# https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/\n",
    "# Monitor =\n",
    "#  - loss\n",
    "#  - accuracy\n",
    "#  - val_loss (val_loss only applies if we use the validation data in the model.fit function)\n",
    "#  - val_binary_crossentropy (val stands for \"validation\" -- as in validation set)\n",
    "#  - precision_metric\n",
    "#  - prec (BAD!!!!)\n",
    "\n",
    "# Mode =\n",
    "#  - min\n",
    "#  - max\n",
    "\n",
    "# min_delta = \n",
    "#  - 0.0001\n",
    "\n",
    "# https://lambdalabs.com/blog/tensorflow-2-0-tutorial-04-early-stopping\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='prec', min_delta=0.0001,patience=1) #TERRIBLE\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='precision_metric', min_delta=0.0001,patience=1) # error\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='precision_metric', patience=3, mode=max) # error\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='precision_metric', patience=3) # error\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='auc', patience=3) # doesn't work\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=3) # BAD\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='mse', min_delta=0.0001, patience=3) # BAD\n",
    "# callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3) # monitors loss if the performance goes down\n",
    "callback = keras.callbacks.EarlyStopping(monitor='precision_metric', patience=1) # monitors loss if the performance goes down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() #This tells us what was in this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/metrics/classification_metrics/#precision-class\n",
    "# Optimizer options:\n",
    "# - sgd (stochastic gradient descent)\n",
    "# - adam?\n",
    "# - rmsprop?\n",
    "\n",
    "# https://keras.io/api/losses/\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses \n",
    "# Loss parameter options: \n",
    "# - binary_crossentropy https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class\n",
    "# - mse \n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "# metrics can be a list, like this: metrics=[\"mae\", \"acc\"]\n",
    "# Metrics parameter options:\n",
    "# - accuracy\n",
    "# - precision https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "# - recall\n",
    "\n",
    "# TO DO: decide if we should use any optimizers (rmsprop, adam, sgd, adagrad, adadelta)\n",
    "from keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "# https://keras.io/api/metrics/classification_metrics/#precision-class\n",
    "\n",
    "# OPTIMIZING FOR PRECISION\n",
    "# Attempt 1:\n",
    "# Note: With a loss function using from_logits=True, thresholds must be 0\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=[keras.metrics.Precision(thresholds=0)]) \n",
    "# Attempt 2:\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[keras.metrics.Precision()])\n",
    "\n",
    "# OPTIMIZING FOR AUC\n",
    "# model.compile(optimizer='sgd',\n",
    "#               loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=[keras.metrics.AUC(from_logits=True)])\n",
    "\n",
    "model.fit(x = x_train, y = y_train, epochs=epochs, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test precision:\", score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score,recall_score\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "\n",
    "# I wonder about this metric? -- it uses spam and ham in the example!\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cf_matrix =confusion_matrix(y_test,y_predict)\n",
    "\n",
    "# ax= plt.subplot()\n",
    "# #annot=True to annotate cells\n",
    "# sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt='');\n",
    "# # labels, title and ticks\n",
    "# ax.set_xlabel('Predicted labels');\n",
    "# ax.set_ylabel('True labels');\n",
    "# ax.set_title('Confusion Matrix');\n",
    "# ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py\n",
    "#------------------------------DBSCAN ------------------------------------\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2) # Computes the distances to all neighbors to help find the ideal eps value\n",
    "nbrs = neigh.fit(x)\n",
    "distances, indices = nbrs.kneighbors(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.ylabel(\"epsilon values\")\n",
    "plt.plot(distances)\n",
    "\n",
    "# The ideal value for epsilon will be near the elbow of this graph\n",
    "# I will choose to search for values between 100 and 200 to find a good epsilon in the next code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: TAKES ABOUT A MINUTE TO RUN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "def find_ideal_eps_and_samples_for_2_clusters():\n",
    "    # eps = [100, 125, 150, 200] # Before feature reduction\n",
    "    eps = [1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
    "    min_samples = [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
    "    for i in eps:\n",
    "        for j in min_samples:\n",
    "            db = DBSCAN(eps=i, min_samples=j).fit(x)\n",
    "            labels = db.labels_\n",
    "            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            if n_clusters_ == 2:\n",
    "                print(\"The ideal eps is: %d\" % i)\n",
    "                print(\"The ideal min_samples is: %d\" % j)\n",
    "                return db\n",
    "\n",
    "# Print out the ideal value for eps and for min_samples parameters:\n",
    "db = find_ideal_eps_and_samples_for_2_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Homogeneity: {metrics.homogeneity_score(y, labels):.3f}\")\n",
    "print(f\"Completeness: {metrics.completeness_score(y, labels):.3f}\")\n",
    "print(f\"V-measure: {metrics.v_measure_score(y, labels):.3f}\")\n",
    "print(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(y, labels):.3f}\")\n",
    "print(\n",
    "    \"Adjusted Mutual Information:\"\n",
    "    f\" {metrics.adjusted_mutual_info_score(y, labels):.3f}\"\n",
    ")\n",
    "print(f\"Silhouette Coefficient: {metrics.silhouette_score(x, labels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = x[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = x[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e93aff87d6e2cf52d5a8e93524dd20fc702e53e0159cbe929e143a46520959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
